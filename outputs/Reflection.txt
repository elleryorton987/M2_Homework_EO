reflection:

From Project 1 to this workflow, I feel like I actually dug through the dataset to understand what was really going on and had better questions to ask my AI assistant. For the first project, I didn’t dig into it very much and didn’t spend too much time on my prompts. This time I was more intentional. I also felt like this Project had more of a rubric to follow, or I had a better idea on what I was going to do. I could use ChatGPT, open up Codex, and see the changes in GitHub. Although with this workflow, I kept having issues trying to create my PR in Codex, and it was more frustrating managing multiple Codex chats.

It’s been nice having an audit trail because it’s easier to document what’s happening, and I can see how this would be super important in information systems or auditing. When I first created the code, I didn’t love the results. You couldn’t tell which course was being rated, it looked bad, and there was random columns. I ended up cleaning the Excel file because the output didn’t have the course name and it could have been formatted better. And after going back and forth for a long time, I think some of the data might have been hallucinating. I double-checked the means and the highest-ranked courses, and the results varied between my GitHub and a new ChatGPT session. It was frustrating trying to get the data to match, and it showed me that I have to verify things myself instead of just trusting the output.

If I had another week, I would play around with it a lot more and try to understand why the results were inconsistent. I would spend more time refining my prompts and testing the workflow to make it more repeatable. I also wish there was a better way to edit prompts inside the same Codex chat instead of having to create a whole new one each time.
